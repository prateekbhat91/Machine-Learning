{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Linear Regression\n",
    "\n",
    "\n",
    "## Introduction:\n",
    "Linear Regression is the simplest form of machine learning method that can be used to identify underlying **linear** relationships in data. It is of the form:$$ y = w_{0} + w_{1}x_{1}+ w_{2}x_{2}+ .... + w_{n}x_{n}$$\n",
    "\n",
    "The aim of linear regression is to find the best fit line that linearly separates the data reduces the error while doing so.\n",
    "\n",
    "## Assumptions of Linear Regression:\n",
    "1. **Linear Relationship:** Linear regression assumes that there is a linear relationship between the predictor variable i.e. X and the target variable i.e. Y.\n",
    "2. **Correlation:** There should be no or little correlation between the input variables.\n",
    "3. **Normal features:** All the input variables should be normally distributed.\n",
    "4. **Autocorrelation of residuals:** The residuals should not be autocorrelated which means that the the current value of the target should not be dependent on the previous observed value.\n",
    "\n",
    "## Objective Function:\n",
    "In ordinary least square linear regression we try to minimize the sum of square of the errors by finding a set of weights. Our objective function is given by \n",
    "$$Cost(w)  = \\frac{1}{2N} \\sum_{i=1}^{N} (y_{i}^{pred} - y_{i}^{true})^{2}$$\n",
    "$$Cost(w) = \\frac{1}{2N} \\sum_{i=1}^{N} (\\sum_{j=1}^{d} w_{j}x_{ij}  - y_{i}^{true})^{2}$$\n",
    "where,\n",
    "* N = Number of data points\n",
    "* d = dimension of a data point\n",
    "\n",
    "In order to minimize $Cost(w)$ we use gradient descent method in which we find the gradient of the objective function with respect to the weights given by: $$\\frac{\\partial Cost(w)}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^{N} (\\sum_{j=1}^{d} w_{j}x_{ij}  - y_{i}^{true})x_{ij} $$\n",
    "\n",
    "## Updating Weights:\n",
    "We update weights using the below equation:\n",
    "$$w^{new}_{j} = w^{previous}_{j} - \\alpha * \\frac{\\partial Cost(w)}{\\partial w} $$\n",
    "$$w^{new}_{j} = w^{previous}_{j} - \\alpha * \\frac{1}{N} \\sum_{i=1}^{N} (\\sum_{j=1}^{d} w_{j}x_{ij}  - y_{i}^{true})x_{ij}$$\n",
    "\n",
    "We continue doing these updates until we reach convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation:\n",
    "Before implementing the code let us first define few functions that we will require.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Import libraries'\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import time\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"Util functions\"\n",
    "def paddingData(data):\n",
    "    '''\n",
    "    :param data: Data to be paded\n",
    "    :return: Padded data with value 1 in the first column\n",
    "    '''\n",
    "    return np.c_[np.ones(data.shape[0]), data]\n",
    "\n",
    "def setWeights(numFeat):\n",
    "    '''\n",
    "    :param numFeat: Total number of features in data\n",
    "    :return: vector of ones of length equal to number of features in data\n",
    "    '''\n",
    "    return np.ones(numFeat).reshape((numFeat, 1))\n",
    "\n",
    "def geterror(true, pred):\n",
    "    '''\n",
    "    :param true: true labels\n",
    "    :param pred: predicted labels\n",
    "    :return: residual\n",
    "    '''\n",
    "    return np.linalg.norm(np.subtract(pred, true))/true.shape[0]\n",
    "\n",
    "def splitData(test_size,cv, numpoints):\n",
    "    #This function from sklearn takes the length of the data and test size and returns bootstrapped indices \n",
    "    #depending on how many boostraps are required\n",
    "    '''\n",
    "    :param test_size: size of the test data required (value between 0 and 1).\n",
    "    :param cv: Number of re-shuffling.\n",
    "    :param numpoints: Total number of data points.\n",
    "    :return: indices of the shuffled splits.\n",
    "    '''\n",
    "    ss = ShuffleSplit(n=numpoints, n_iter=cv, test_size=test_size, random_state=32)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Gradient Descent Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchLinearRegression():\n",
    "\n",
    "    def __init__(self,tol):\n",
    "        '''\n",
    "        :param alpha: learning rate\n",
    "        :param tol: tolerance level for convergence\n",
    "        '''\n",
    "        self.tolerance = tol\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self,Xtrain,ytrain):\n",
    "        start = time.time()\n",
    "        print \"Running Batch Gradient Descent\"\n",
    "        runs = 0\n",
    "        'Do a padding of one'\n",
    "        Xtrain = paddingData(Xtrain)\n",
    "        self.weights = setWeights(Xtrain.shape[1])\n",
    "        while True:\n",
    "            runs += 1\n",
    "            loss = np.dot(Xtrain, self.weights) - ytrain\n",
    "            gradient = np.dot(Xtrain.transpose(), loss) / Xtrain.shape[0]\n",
    "            temp = self.weights - (1/runs) * gradient\n",
    "            step = np.linalg.norm(np.subtract(self.weights, temp))\n",
    "            self.weights = temp\n",
    "            #print \"Step = \", step\n",
    "            if step < self.tolerance:\n",
    "                break\n",
    "        end = time.time()\n",
    "        print 'Time taken to fit the data:', end-start\n",
    "        print 'Number of passes over data:',runs\n",
    "\n",
    "    def predict(self,Xtest):\n",
    "        Xtest = paddingData(Xtest)\n",
    "        return np.dot(Xtest, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    def __init__(self,tol,Lambda):\n",
    "        '''\n",
    "        :param alpha: learning rate\n",
    "        :param tol: tolerance level for convergence\n",
    "        :param Lambda: regularization parameter\n",
    "        '''\n",
    "        self.tolerance = tol\n",
    "        self.Lambda = Lambda\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self,Xtrain,ytrain):\n",
    "        start = time.time()\n",
    "        print \"Running Ridge Regression\"\n",
    "        runs = 0\n",
    "        'Do a padding of one'\n",
    "        Xtrain = paddingData(Xtrain)\n",
    "        self.weights = setWeights(Xtrain.shape[1])\n",
    "        while True:\n",
    "            runs += 1\n",
    "            loss = np.dot(Xtrain, self.weights) - ytrain\n",
    "            gradient = np.dot(Xtrain.transpose(), loss) / Xtrain.shape[0]\n",
    "            temp = (1 - 2 * self.Lambda * (1/runs)) * self.weights - (1/runs) * gradient\n",
    "            step = np.linalg.norm(np.subtract(self.weights, temp))\n",
    "            self.weights = temp\n",
    "            #print step\n",
    "            if step < self.tolerance:\n",
    "                break\n",
    "        end = time.time()\n",
    "        print 'Time taken to fit the data:', end-start\n",
    "        print 'Number of passes over data:',runs\n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        Xtest = paddingData(Xtest)\n",
    "        return np.dot(Xtest, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LassoRegresion():\n",
    "\n",
    "    def __init__(self,tol,Lambda):\n",
    "        '''\n",
    "        :param tol: tolerance level for convergence\n",
    "        :param Lambda: regularization parameter\n",
    "        '''\n",
    "        self.tolerance = tol\n",
    "        self.Lambda = Lambda\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self,Xtrain,ytrain):\n",
    "        start = time.time()\n",
    "        runs = 0\n",
    "        print \"Running Lasso Regression\"\n",
    "        'Do a padding of one'\n",
    "        Xtrain = paddingData(Xtrain)\n",
    "        self.weights = setWeights(Xtrain.shape[1])\n",
    "        while True:\n",
    "            runs += 1\n",
    "            temp = copy.copy(self.weights)\n",
    "            for i in range(1,Xtrain.shape[1]):\n",
    "                Xnew = np.delete(Xtrain, [i], axis=1)\n",
    "                Wnew = np.delete(self.weights, [i], axis=0)\n",
    "                residual = np.linalg.norm(np.subtract(ytrain,np.dot(Xnew, Wnew)))/Xtrain.shape[0]\n",
    "                if residual < -self.Lambda/2:\n",
    "                    self.weights[i] = residual + self.Lambda/2\n",
    "                elif residual < self.Lambda/2 and residual > -self.Lambda/2:\n",
    "                    self.weights[i] = 0\n",
    "                else:\n",
    "                    self.weights[i] = residual - self.Lambda/2\n",
    "\n",
    "            step = np.linalg.norm(np.subtract(self.weights, temp))\n",
    "            #print \"Step = \", step\n",
    "            if step < self.tolerance:\n",
    "                break\n",
    "        end = time.time()\n",
    "        print 'Time taken to fit the data:', end-start\n",
    "        print 'Number of passes over data:',runs\n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        Xtest = paddingData(Xtest)\n",
    "        return np.dot(Xtest, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data:\n",
    "**Now let us load our data and modify it according to the arguments of our algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "numSamples, numFeat = boston.data.shape\n",
    "ss = splitData(test_size=0.25,cv=1, numpoints=numSamples)\n",
    "for train_index, test_index in ss:\n",
    "    Xtrain = boston.data[train_index, :]\n",
    "    ytrain = boston.target[train_index].reshape((train_index.shape[0], 1))\n",
    "    Xtest = boston.data[test_index, :]\n",
    "    ytest = boston.target[test_index].reshape((test_index.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Normalize the data to zero mean and unit variance'\n",
    "scalar = StandardScaler()\n",
    "Xtrain = scalar.fit_transform(Xtrain)\n",
    "Xtest = scalar.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our algorithms:\n",
    "I have also compared the performance of these three algorithms with native sklearn algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Import sklearn regressors'\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Batch Gradient Descent\n",
      "Time taken to fit the data: 0.0514540672302\n",
      "Number of passes over data: 2249\n",
      "Error: 0.459311346147\n",
      "\n",
      "Error from running sklearn: 0.453497809748\n"
     ]
    }
   ],
   "source": [
    "'Run Batch Gradient Descent'\n",
    "batchreg = BatchLinearRegression(tol=0.0001)\n",
    "batchreg.fit(Xtrain, ytrain)\n",
    "pred = batchreg.predict(Xtest)\n",
    "pred = pred.reshape((pred.shape[0], 1))\n",
    "print 'Error:',geterror(ytest, pred)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(Xtrain,ytrain)\n",
    "pred = reg.predict(Xtest)\n",
    "print '\\nError from running sklearn:',geterror(pred,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Lasso Regression\n",
      "Time taken to fit the data: 0.00445985794067\n",
      "Number of passes over data: 4\n",
      "Error: 2.17753026667\n",
      "\n",
      "Error from running sklearn: 11.8773268599\n"
     ]
    }
   ],
   "source": [
    "'Run Lasso Regression'\n",
    "lass = LassoRegresion(tol = 0.0001,Lambda=0.001)\n",
    "lass.fit(Xtrain,ytrain)\n",
    "pred = lass.predict(Xtest)\n",
    "pred = pred.reshape((pred.shape[0], 1))\n",
    "print 'Error:',geterror(ytest, pred)\n",
    "\n",
    "reg = LassoLars(alpha=0.001)\n",
    "reg.fit(Xtrain,ytrain)\n",
    "pred = reg.predict(Xtest)\n",
    "print '\\nError from running sklearn:',geterror(pred,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Ridge Regression\n",
      "Time taken to fit the data: 0.0685868263245\n",
      "Number of passes over data: 2219\n",
      "Error: 0.459630180955\n",
      "\n",
      "Error from running sklearn: 0.453498270648\n"
     ]
    }
   ],
   "source": [
    "'Run Ridge Regression'\n",
    "rid = RidgeRegression(tol=0.0001,Lambda=0.001)\n",
    "rid.fit(Xtrain,ytrain)\n",
    "pred = rid.predict(Xtest)\n",
    "pred = pred.reshape((pred.shape[0], 1))\n",
    "print 'Error:',geterror(ytest, pred)\n",
    "\n",
    "reg = Ridge(tol=0.0001,alpha=0.001)\n",
    "reg.fit(Xtrain,ytrain)\n",
    "pred = reg.predict(Xtest)\n",
    "print '\\nError from running sklearn:',geterror(pred,ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
