{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression:\n",
    "\n",
    "## Introduction:\n",
    "Logistic regression is a linear classifier which belongs to the family of discriminative machine learning models. It tries to learn $P(y|x)$ from data and does prediction following a linear threshold unit i.e.\n",
    "$$h(x) =  \\begin{cases} \n",
    "      1 & w_{1}x_{1}+w_{2}x_{2}+..+w_{d}x_{d}\\geqslant 0 \\\\\n",
    "      0 & otherwise\n",
    "   \\end{cases}$$\n",
    "   \n",
    "Logistic regression learns the weight of each feature in the data set and uses sigmoid function to transform the predicted values into probabilities. A linear function can have a range of (${-\\infty,+\\infty}$) but it is transformed to [0,1] using the sigmoid function.\n",
    "$$sigmoid(wx) = \\frac{1}{1+ e^{-wx}}$$\n",
    "\n",
    "The cost function for logistic regresion is given by:\n",
    "$$J(w) = \\sum_{i=1}^{N} (1-y_{i})\\log(1-p(x,w) + y_{i}p(x,w)$$\n",
    "\n",
    "the gradient is given by:\n",
    "$$\\frac{\\partial J(w)}{\\partial w_{j}} = \\sum_{i=1}^{N}(y_{i}-p(x,w))x_{ij}$$\n",
    "\n",
    "and the weight update is done:\n",
    "$$w_{t+1} = w_{t} + \\alpha * gradient$$\n",
    "\n",
    "where, $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Python Implementation:\n",
    "**Let us first implement few functions that we will need before implementing logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Import libraries'\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import time\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Utils functions'\n",
    "def paddingData(data):\n",
    "    '''\n",
    "    :param data: Data to be paded\n",
    "    :return: Padded data with value 1 in the first column\n",
    "    '''\n",
    "    return np.c_[np.ones(data.shape[0]), data]\n",
    "\n",
    "def setWeights(numFeat):\n",
    "    '''\n",
    "    :param numFeat: Total number of features in data\n",
    "    :return: vector of ones of length equal to number of features in data\n",
    "    '''\n",
    "    return np.ones(numFeat).reshape((numFeat, 1))\n",
    "\n",
    "def splitData(test_size,cv, numpoints):\n",
    "    #This function from sklearn takes the length of the data and test size and returns bootstrapped indices \n",
    "    #depending on how many boostraps are required\n",
    "    '''\n",
    "    :param test_size: size of the test data required (value between 0 and 1).\n",
    "    :param cv: Number of re-shuffling.\n",
    "    :param numpoints: Total number of data points.\n",
    "    :return: indices of the shuffled splits.\n",
    "    '''\n",
    "    ss = ShuffleSplit(n=numpoints, n_iter=cv, test_size=test_size, random_state=32)\n",
    "    return ss\n",
    "\n",
    "def calAccuracy(pred,ytest):\n",
    "    '''\n",
    "    :param pred: vector containing all the predicted classes\n",
    "    :param ytest: vector containing all the true classes\n",
    "    :return: accuracy of classification\n",
    "    '''\n",
    "    count = 0\n",
    "    for i,j in izip(pred,ytest):\n",
    "        if i==j:\n",
    "            count +=1\n",
    "    return count/(len(ytest))\n",
    "\n",
    "def sigmoid(a):\n",
    "    '''\n",
    "    :param a: vector (w.x)\n",
    "    :return: sigmoid transfer of the value\n",
    "    '''\n",
    "    return 1/(1+np.exp(-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Implements logistic regression'\n",
    "class logisticregression():\n",
    "\n",
    "    def __init__(self,tol=0.0001,alpha = 0.01):\n",
    "        '''\n",
    "        :param weights: weight vector\n",
    "        :param tol: tolerance with the default value of 0.0001\n",
    "        :param alpha: learning rate with the default value of 0.01\n",
    "        '''\n",
    "        self.weights = None\n",
    "        self.tolerance = tol\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self,Xtrain,ytrain):\n",
    "        'Start time'\n",
    "        start = time.time()\n",
    "        'Padding of input data'\n",
    "        Xtrain  = paddingData(Xtrain)\n",
    "        self.weights = setWeights(Xtrain.shape[1])\n",
    "        'save the number passes over data'\n",
    "        run = 0\n",
    "        while True:\n",
    "            run +=1\n",
    "            'predict using the current weight'\n",
    "            predict = np.dot(Xtrain,self.weights)\n",
    "            'calculate the probability of data point belonging to class 1(in case of binary)'\n",
    "            prob = sigmoid(predict)\n",
    "            'calculate the error'\n",
    "            error = ytrain - prob\n",
    "            gradient = np.dot(error.T ,Xtrain) / ytrain.shape[0]\n",
    "            temp = self.weights + self.alpha* gradient.T\n",
    "            step = np.linalg.norm(np.subtract(self.weights, temp))\n",
    "            self.weights = temp\n",
    "            if step < self.tolerance:\n",
    "                break\n",
    "        end = time.time()\n",
    "        print 'Time taken to fit data:',end-start\n",
    "        print 'Number of passes over data:', run\n",
    "                \n",
    "                \n",
    "    def predict(self,Xtest):\n",
    "        'Pad the test data'\n",
    "        Xtest = paddingData(Xtest)\n",
    "        'predict using the learned weights and convert it into probability'\n",
    "        pred = sigmoid(Xtest.dot(self.weights))\n",
    "        pred[pred > 0.5  ] = 1\n",
    "        pred[pred <= 0.5] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "'''\n",
    "Classes\t2\n",
    "Samples per class\t212(M),357(B)\n",
    "Samples total\t569\n",
    "Dimensionality\t30\n",
    "Features\treal, positive\n",
    "'''\n",
    "breastcancer = load_breast_cancer()\n",
    "numSamples, numFeat = breastcancer.data.shape\n",
    "ss = splitData(test_size=0.25,cv=1, numpoints=numSamples)\n",
    "for train_index, test_index in ss:\n",
    "    Xtrain = breastcancer.data[train_index, :]\n",
    "    ytrain = breastcancer.target[train_index].reshape((train_index.shape[0], 1))\n",
    "    Xtest = breastcancer.data[test_index, :]\n",
    "    ytest = breastcancer.target[test_index].reshape((test_index.shape[0], 1))\n",
    "\n",
    "'Normalize the data to zero mean and unit variance'\n",
    "scalar = StandardScaler()\n",
    "Xtrain = scalar.fit_transform(Xtrain)\n",
    "Xtest = scalar.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running our algorithm:\n",
    "I have also compared the performance of our logistic regression implementation with sklearn logistic regression as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit data: 0.741101980209\n",
      "Number of passes over data: 10706\n",
      "Accuracy: 99.3006993007\n"
     ]
    }
   ],
   "source": [
    "clf = logisticregression()\n",
    "clf.fit(Xtrain,ytrain)\n",
    "pred = clf.predict(Xtest)\n",
    "print 'Accuracy:',calAccuracy(pred,ytest)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.3006993007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.local/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(Xtrain,ytrain)\n",
    "p = lgr.predict(Xtest)\n",
    "print 'Accuracy:',calAccuracy(p,ytest)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "Our algorithm gives the same accuracy as sklearn, accuracy is a good measure here because the class distribuion is almost equal. The same idea can be extended to multi class classification using one-vs-all method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Refrences:\n",
    "1. [Wiki](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "2. [Penn State: STAT 504](https://onlinecourses.science.psu.edu/stat504/node/149)\n",
    "3. My class notes and slides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
