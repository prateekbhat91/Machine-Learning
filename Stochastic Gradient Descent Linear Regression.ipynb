{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "In gradient descent we calculate the gradient over the whole dataset and update our weights accordingly while in stochastic gradient descent we perform update over individual data points making SGD much faster.The update over each data point will cause a lot of fluctuations in the weight vector, however decreasing the learning rate over time SGD shows the same convergence behaviour as batch gradient descent. In order to get the best of both worlds we can use Mini-Batch gradient descent.\n",
    "\n",
    "# Algorithm:\n",
    "1. Initalize weight vector.\n",
    "2. for i = 1 to number of epochs do\n",
    "    3. shuffle data points\n",
    "    4. for j = 1 to number of data points do\n",
    "        5. calculate the gradient \n",
    "        6. decrease the learning rate\n",
    "        7. update the weight vector\n",
    "8. return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Import libraries'\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import time\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before implementing the code let us first define few functions that we will need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"Util functions\"\n",
    "def paddingData(data):\n",
    "    '''\n",
    "    :param data: Data to be paded\n",
    "    :return: Padded data with value 1 in the first column\n",
    "    '''\n",
    "    return np.c_[np.ones(data.shape[0]), data]\n",
    "\n",
    "def setWeights(numFeat):\n",
    "    '''\n",
    "    :param numFeat: Total number of features in data\n",
    "    :return: vector of ones of length equal to number of features in data\n",
    "    '''\n",
    "    return np.ones(numFeat).reshape((numFeat, 1))\n",
    "\n",
    "def geterror(true, pred):\n",
    "    '''\n",
    "    :param true: true labels\n",
    "    :param pred: predicted labels\n",
    "    :return: residual\n",
    "    '''\n",
    "    return np.linalg.norm(np.subtract(pred, true))/true.shape[0]\n",
    "\n",
    "def splitData(test_size,cv, numpoints):\n",
    "    #This function from sklearn takes the length of the data and test size and returns bootstrapped indices \n",
    "    #depending on how many boostraps are required\n",
    "    '''\n",
    "    :param test_size: size of the test data required (value between 0 and 1).\n",
    "    :param cv: Number of re-shuffling.\n",
    "    :param numpoints: Total number of data points.\n",
    "    :return: indices of the shuffled splits.\n",
    "    '''\n",
    "    ss = ShuffleSplit(n=numpoints, n_iter=cv, test_size=test_size, random_state=32)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Implements SGD with l2 regularization'\n",
    "class stochasticgradientdescent():\n",
    "\n",
    "    def __init__(self,alpha,epoch,Lambda):\n",
    "        '''\n",
    "        :param alpha: learning rate\n",
    "        :param epoch: number of passes over data\n",
    "        :param Lambda: regularization parameter\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.Lambda = Lambda\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self,Xtrain,ytrain):\n",
    "        start = time.time()\n",
    "        print \"Running Stochastic Gradient Descent\"\n",
    "        'Do a padding of one'\n",
    "        Xtrain = paddingData(Xtrain)\n",
    "        self.weights = setWeights(Xtrain.shape[1])\n",
    "        'Random seed for result reproducibility'\n",
    "        np.random.seed(32)\n",
    "        for _ in range(self.epoch):\n",
    "            'Generate random integers in range of number of rows in data'\n",
    "            ite = np.random.choice(a=Xtrain.shape[0], size=Xtrain.shape[0], replace=False)\n",
    "            for i in ite:\n",
    "                oneData = Xtrain[i, :].reshape((1, Xtrain.shape[1]))\n",
    "                oneLabel = ytrain[i,:]\n",
    "                loss = np.dot(oneData, self.weights) - oneLabel\n",
    "                gradient = np.dot(oneData.transpose(), loss)\n",
    "                self.weights = (1 - 2 * self.Lambda * self.alpha) * self.weights - self.alpha * gradient\n",
    "        end = time.time()\n",
    "        print 'Time taken to fit data:', end-start\n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        Xtest = paddingData(Xtest)\n",
    "        return np.dot(Xtest, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data:\n",
    "We will use the diabetes dataset from sklearn to test our algorithm. This data set has 10 continious features and 442 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "numSamples, numFeat = diabetes.data.shape\n",
    "ss = splitData(test_size=0.25,cv=1, numpoints=numSamples)\n",
    "for train_index, test_index in ss:\n",
    "    Xtrain = diabetes.data[train_index, :]\n",
    "    ytrain = diabetes.target[train_index].reshape((train_index.shape[0], 1))\n",
    "    Xtest = diabetes.data[test_index, :]\n",
    "    ytest = diabetes.target[test_index].reshape((test_index.shape[0], 1))\n",
    "# from sklearn.datasets import load_boston\n",
    "# boston = load_boston()\n",
    "# numSamples, numFeat = boston.data.shape\n",
    "# ss = splitData(test_size=0.25,cv=1, numpoints=numSamples)\n",
    "# for train_index, test_index in ss:\n",
    "#     Xtrain = boston.data[train_index, :]\n",
    "#     ytrain = boston.target[train_index].reshape((train_index.shape[0], 1))\n",
    "#     Xtest = boston.data[test_index, :]\n",
    "#     ytest = boston.target[test_index].reshape((test_index.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Normalize the data to zero mean and unit variance'\n",
    "scalar = StandardScaler()\n",
    "Xtrain = scalar.fit_transform(Xtrain)\n",
    "Xtest = scalar.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running our algorithm:\n",
    "I have also compared the performance of our implemented SGD with sklearn SGDRegressor as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stochastic Gradient Descent\n",
      "Time taken to fit data: 0.588402986526\n",
      "Error: 5.10859016007\n"
     ]
    }
   ],
   "source": [
    "ourSGD = stochasticgradientdescent(alpha = 0.001,epoch = 200,Lambda = 0.0001)\n",
    "ourSGD.fit(Xtrain,ytrain)\n",
    "pred = ourSGD.predict(Xtest)\n",
    "pred = pred.reshape((pred.shape[0], 1))\n",
    "print 'Error:',geterror(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using sklearn SGDRegressor: 5.05439165818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.local/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(Xtrain,ytrain)\n",
    "pred1 = sgd.predict(Xtest)\n",
    "pred1 = pred1.reshape((pred1.shape[0], 1))\n",
    "print 'Error using sklearn SGDRegressor:',geterror(ytest, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "Our algorithm almost gives the same error as sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refrences:\n",
    "My class notes and slides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
